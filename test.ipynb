{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Coefficient: tensor([[-0.0098]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, value, key, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        value = value.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        key = key.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(value)\n",
    "        keys = self.keys(key)\n",
    "        queries = self.queries(query)\n",
    "\n",
    "        # Calculate the attention scores\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        attention = F.softmax(attention / (self.embed_size ** (1 / 2)), dim=-1)\n",
    "\n",
    "        # Apply the attention scores to the values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        # Apply a final linear layer\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class ModalAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, modalities):\n",
    "        super(ModalAttention, self).__init__()\n",
    "        self.modalities = modalities\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.fc = nn.Linear(len(modalities) * embed_size, 1)\n",
    "\n",
    "    def forward(self, fusion, *modalities):\n",
    "        attention_weights = []\n",
    "        for modality in modalities:\n",
    "            attention_output = self.attention(modality, modality, fusion)\n",
    "            attention_weights.append(torch.mean(attention_output, dim=1))\n",
    "\n",
    "        # Concatenate the attention weights and pass through a linear layer to get the final weight coefficient\n",
    "        concatenated_weights = torch.cat(attention_weights, dim=1)\n",
    "        weight_coefficient = self.fc(concatenated_weights)\n",
    "        return weight_coefficient\n",
    "\n",
    "# Example usage\n",
    "embed_size = 128\n",
    "heads = 4\n",
    "modalities = ['text', 'video', 'audio']\n",
    "\n",
    "# Create random tensors for fusion and modalities\n",
    "fusion = torch.randn((1, 10, embed_size))  # (batch_size, sequence_length, embed_size)\n",
    "text = torch.randn((1, 10, embed_size))\n",
    "video = torch.randn((1, 10, embed_size))\n",
    "audio = torch.randn((1, 10, embed_size))\n",
    "\n",
    "# Initialize the ModalAttention module\n",
    "modal_attention = ModalAttention(embed_size, heads, modalities)\n",
    "\n",
    "# Compute the weight coefficient\n",
    "weight_coefficient = modal_attention(fusion, text, video, audio)\n",
    "print(\"Weight Coefficient:\", weight_coefficient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: tensor([[0.3436, 0.3216, 0.3404, 0.3017, 0.3263, 0.3533, 0.3364, 0.3165, 0.3373,\n",
      "         0.3369, 0.3503, 0.3557, 0.3588, 0.3330, 0.3438, 0.3302, 0.3115, 0.3377,\n",
      "         0.3180, 0.3148, 0.3471, 0.3427, 0.3404, 0.3195, 0.3345, 0.3108, 0.3255,\n",
      "         0.3080, 0.3442, 0.3329, 0.3338, 0.3211],\n",
      "        [0.3216, 0.3352, 0.3430, 0.3357, 0.3457, 0.3154, 0.3447, 0.3612, 0.3333,\n",
      "         0.3548, 0.3216, 0.3224, 0.3232, 0.3321, 0.3064, 0.3527, 0.3576, 0.3256,\n",
      "         0.3411, 0.3407, 0.3162, 0.3099, 0.2915, 0.3318, 0.3333, 0.3377, 0.3436,\n",
      "         0.3419, 0.3044, 0.3463, 0.3319, 0.3288],\n",
      "        [0.3348, 0.3433, 0.3165, 0.3626, 0.3280, 0.3313, 0.3189, 0.3222, 0.3294,\n",
      "         0.3084, 0.3281, 0.3220, 0.3179, 0.3350, 0.3498, 0.3171, 0.3309, 0.3367,\n",
      "         0.3409, 0.3445, 0.3367, 0.3474, 0.3681, 0.3487, 0.3322, 0.3515, 0.3309,\n",
      "         0.3500, 0.3514, 0.3209, 0.3344, 0.3501]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModalAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(ModalAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query = nn.Parameter(torch.randn(embed_dim))\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, fusion, text, video, audio):\n",
    "        # 计算每个模态的键向量\n",
    "        key_text = self.key(text)\n",
    "        key_video = self.key(video)\n",
    "        key_audio = self.key(audio)\n",
    "\n",
    "        # 计算每个模态与融合模态之间的相似度\n",
    "        sim_text = F.cosine_similarity(self.query, key_text, dim=-1)\n",
    "        sim_video = F.cosine_similarity(self.query, key_video, dim=-1)\n",
    "        sim_audio = F.cosine_similarity(self.query, key_audio, dim=-1)\n",
    "\n",
    "        # 将相似度转换为权重\n",
    "        weights = F.softmax(torch.stack([sim_text, sim_video, sim_audio]), dim=0)\n",
    "\n",
    "        return weights\n",
    "\n",
    "# 示例使用\n",
    "embed_dim = 64\n",
    "batch_size = 32\n",
    "\n",
    "model = ModalAttention(embed_dim=embed_dim)\n",
    "fusion = torch.rand(batch_size, embed_dim)\n",
    "text = torch.rand(batch_size, embed_dim)\n",
    "video = torch.rand(batch_size, embed_dim)\n",
    "audio = torch.rand(batch_size, embed_dim)\n",
    "\n",
    "weights = model(fusion, text, video, audio)\n",
    "\n",
    "print(\"Weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight (text): tensor([0.4532, 0.3623, 0.2640, 0.2711, 0.2826, 0.4319, 0.0107, 0.6332, 0.2323,\n",
      "        0.2703, 0.4177, 0.3813, 0.2058, 0.4034, 0.5191, 0.2613, 0.2060, 0.6612,\n",
      "        0.1712, 0.2346, 0.2904, 0.5253, 0.3491, 0.5234, 0.2375, 0.3362, 0.3451,\n",
      "        0.1626, 0.2903, 0.0746, 0.2338, 0.4558], grad_fn=<DivBackward0>)\n",
      "Weight (video): tensor([0.3646, 0.2566, 0.1637, 0.3269, 0.2450, 0.3317, 0.6042, 0.2875, 0.3048,\n",
      "        0.3187, 0.2695, 0.5047, 0.4971, 0.2498, 0.2946, 0.4898, 0.4071, 0.1398,\n",
      "        0.3240, 0.5675, 0.3254, 0.2062, 0.2386, 0.1410, 0.1482, 0.4671, 0.2906,\n",
      "        0.4586, 0.4693, 0.3450, 0.3958, 0.3312], grad_fn=<DivBackward0>)\n",
      "Weight (audio): tensor([0.1822, 0.3811, 0.5723, 0.4020, 0.4724, 0.2364, 0.3851, 0.0793, 0.4629,\n",
      "        0.4110, 0.3128, 0.1141, 0.2971, 0.3468, 0.1863, 0.2490, 0.3869, 0.1990,\n",
      "        0.5048, 0.1979, 0.3842, 0.2685, 0.4123, 0.3356, 0.6143, 0.1967, 0.3643,\n",
      "        0.3788, 0.2404, 0.5804, 0.3704, 0.2131], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModalAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(ModalAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query = nn.Parameter(torch.randn(embed_dim))\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, fusion, text, video, audio):\n",
    "        # 计算每个模态的键向量\n",
    "        key_text = self.key(text)\n",
    "        key_video = self.key(video)\n",
    "        key_audio = self.key(audio)\n",
    "\n",
    "        # 计算每个模态与融合模态之间的相似度\n",
    "        sim_text = F.cosine_similarity(self.query, key_text, dim=-1)\n",
    "        sim_video = F.cosine_similarity(self.query, key_video, dim=-1)\n",
    "        sim_audio = F.cosine_similarity(self.query, key_audio, dim=-1)\n",
    "\n",
    "        # 对相似度进行归一化处理，得到每个模态的权重\n",
    "        total_sim = sim_text + sim_video + sim_audio\n",
    "        weight_text = sim_text / total_sim\n",
    "        weight_video = sim_video / total_sim\n",
    "        weight_audio = sim_audio / total_sim\n",
    "\n",
    "        return weight_text, weight_video, weight_audio\n",
    "\n",
    "# 示例使用\n",
    "embed_dim = 64\n",
    "batch_size = 32\n",
    "\n",
    "model = ModalAttention(embed_dim=embed_dim)\n",
    "fusion = torch.rand(batch_size, embed_dim)\n",
    "text = torch.rand(batch_size, embed_dim)\n",
    "video = torch.rand(batch_size, embed_dim)\n",
    "audio = torch.rand(batch_size, embed_dim)\n",
    "\n",
    "weight_text, weight_video, weight_audio = model(fusion, text, video, audio)\n",
    "\n",
    "print(\"Weight (text):\", weight_text)\n",
    "print(\"Weight (video):\", weight_video)\n",
    "print(\"Weight (audio):\", weight_audio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gated Text weight: tensor([[0.3326],\n",
      "        [0.3327],\n",
      "        [0.3327],\n",
      "        [0.3328],\n",
      "        [0.3328]], grad_fn=<MeanBackward1>)\n",
      "Gated Video weight: tensor([[0.3303],\n",
      "        [0.3303],\n",
      "        [0.3303],\n",
      "        [0.3299],\n",
      "        [0.3303]], grad_fn=<MeanBackward1>)\n",
      "Gated Audio weight: tensor([[0.3372],\n",
      "        [0.3369],\n",
      "        [0.3370],\n",
      "        [0.3372],\n",
      "        [0.3370]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "class AdaptiveModalityAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(AdaptiveModalityAttention, self).__init__()\n",
    "        self.multihead_attention = MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(30, embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size, 3),  # 3 modalities\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, fusion, text, video, audio):\n",
    "        # Transpose inputs to match the expected shape for MultiheadAttention\n",
    "        fusion = fusion.transpose(0, 1)  # (seq_len, batch_size, embed_size)\n",
    "        text = text.transpose(0, 1)  # (seq_len, batch_size, embed_size)\n",
    "        video = video.transpose(0, 1)  # (seq_len, batch_size, embed_size)\n",
    "        audio = audio.transpose(0, 1)  # (seq_len, batch_size, embed_size)\n",
    "\n",
    "        # Calculate attention weights for each modality\n",
    "        text_attention, _ = self.multihead_attention(fusion, text, text)\n",
    "        video_attention, _ = self.multihead_attention(fusion, video, video)\n",
    "        audio_attention, _ = self.multihead_attention(fusion, audio, audio)\n",
    "        \n",
    "        # Concatenate the attention weights across the sequence length dimension\n",
    "        weights = torch.cat([text_attention, video_attention, audio_attention], dim=0)  # (3 * seq_len, batch_size, embed_size)\n",
    "        weights = weights.permute(1, 2, 0)  # (batch_size, embed_size, 3 * seq_len)\n",
    "        \n",
    "        # Pass the concatenated weights through the gate\n",
    "        gated_weights = self.gate(weights)  # (batch_size, embed_size, 3)\n",
    "\n",
    "        # Separate the gated weights and take the mean across the embed_size dimension\n",
    "        gated_text_weight = torch.mean(gated_weights[:, :, 0], dim=1, keepdim=True)  # (batch_size, 1)\n",
    "        gated_video_weight = torch.mean(gated_weights[:, :, 1], dim=1, keepdim=True)  # (batch_size, 1)\n",
    "        gated_audio_weight = torch.mean(gated_weights[:, :, 2], dim=1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        return gated_text_weight, gated_video_weight, gated_audio_weight\n",
    "\n",
    "# Example usage\n",
    "embed_size = 256\n",
    "heads = 8\n",
    "adaptive_attention = AdaptiveModalityAttention(embed_size, heads)\n",
    "fusion = torch.rand(5, 10, embed_size)  # (batch_size, seq_len, embed_size)\n",
    "text = torch.rand(5, 10, embed_size)  # (batch_size, seq_len, embed_size)\n",
    "video = torch.rand(5, 10, embed_size)  # (batch_size, seq_len, embed_size)\n",
    "audio = torch.rand(5, 10, embed_size)  # (batch_size, seq_len, embed_size)\n",
    "\n",
    "gated_text_weight, gated_video_weight, gated_audio_weight = adaptive_attention(fusion, text, video, audio)\n",
    "print(\"Gated Text weight:\", gated_text_weight)\n",
    "print(\"Gated Video weight:\", gated_video_weight)\n",
    "print(\"Gated Audio weight:\", gated_audio_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m audio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, seq_len, feature_size)\n\u001b[1;32m     59\u001b[0m modal_attention \u001b[38;5;241m=\u001b[39m ModalAttention(feature_size)\n\u001b[0;32m---> 60\u001b[0m text_constant, video_constant, audio_constant \u001b[38;5;241m=\u001b[39m \u001b[43mmodal_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_constant)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(video_constant)\n",
      "File \u001b[0;32m~/anaconda3/envs/MSA/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MSA/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 45\u001b[0m, in \u001b[0;36mModalAttention.forward\u001b[0;34m(self, fusion, text, video, audio)\u001b[0m\n\u001b[1;32m     42\u001b[0m normalized_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Splitting the normalized weights into separate constants for each modality\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m text_constant, video_constant, audio_constant \u001b[38;5;241m=\u001b[39m normalized_weights\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text_constant, video_constant, audio_constant\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_f.size():torch.Size([2, 5, 3])\n",
      "text_weights:torch.Size([2, 5, 5])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m audio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, seq_len, feature_size)\n\u001b[1;32m     41\u001b[0m modal_attention \u001b[38;5;241m=\u001b[39m ModalAttention(feature_size, num_heads)\n\u001b[0;32m---> 42\u001b[0m text_constant, video_constant, audio_constant \u001b[38;5;241m=\u001b[39m \u001b[43mmodal_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_constant)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(video_constant)\n",
      "File \u001b[0;32m~/anaconda3/envs/MSA/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MSA/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[49], line 26\u001b[0m, in \u001b[0;36mModalAttention.forward\u001b[0;34m(self, fusion, text, video, audio)\u001b[0m\n\u001b[1;32m     23\u001b[0m normalized_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Splitting the normalized weights into separate constants for each modality\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m text_constant, video_constant, audio_constant \u001b[38;5;241m=\u001b[39m normalized_weights\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text_constant, video_constant, audio_constant\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModalAttention(nn.Module):\n",
    "    def __init__(self, feature_size, num_heads):\n",
    "        super(ModalAttention, self).__init__()\n",
    "        self.multi_head_attention = nn.MultiheadAttention(embed_dim=feature_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, fusion, text, video, audio):\n",
    "        t_f, text_weights = self.multi_head_attention(fusion, text, text)\n",
    "        _, video_weights = self.multi_head_attention(fusion, video, video)\n",
    "        _, audio_weights = self.multi_head_attention(fusion, audio, audio)\n",
    "        print(f\"t_f.size():{t_f.size()}\")\n",
    "        print(f\"text_weights:{text_weights.size()}\")\n",
    "        # Summing up the attention weights across the sequence length dimension\n",
    "        text_weights = text_weights.sum(dim=-2)\n",
    "        video_weights = video_weights.sum(dim=-2)\n",
    "        audio_weights = audio_weights.sum(dim=-2)\n",
    "\n",
    "        # Concatenating the weights and applying softmax to normalize\n",
    "        weights = torch.cat([text_weights, video_weights, audio_weights], dim=-1)\n",
    "        normalized_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # Splitting the normalized weights into separate constants for each modality\n",
    "        text_constant, video_constant, audio_constant = normalized_weights.split(1, dim=-1)\n",
    "\n",
    "        return text_constant, video_constant, audio_constant\n",
    "\n",
    "# Example usage\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "feature_size = 3\n",
    "num_heads = 1\n",
    "\n",
    "fusion = torch.randn(batch_size, seq_len, feature_size)\n",
    "text = torch.randn(batch_size, seq_len, feature_size)\n",
    "video = torch.randn(batch_size, seq_len, feature_size)\n",
    "audio = torch.randn(batch_size, seq_len, feature_size)\n",
    "\n",
    "modal_attention = ModalAttention(feature_size, num_heads)\n",
    "text_constant, video_constant, audio_constant = modal_attention(fusion, text, video, audio)\n",
    "\n",
    "print(text_constant)\n",
    "print(video_constant)\n",
    "print(audio_constant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'RANK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRANK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWORLD_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(rank,world_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/MSA/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RANK'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "rank = int(os.environ[\"RANK\"])\n",
    "world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "print(rank,world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
